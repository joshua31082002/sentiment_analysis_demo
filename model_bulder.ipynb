{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LOADING REQUIRED DATASETS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import re\n",
    "from sklearn.pipeline import Pipeline\n",
    "import spacy\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "word_lemmatizer = WordNetLemmatizer()\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "from sklearn.base import TransformerMixin,BaseEstimator\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "from sklearn.ensemble import GradientBoostingClassifier,AdaBoostClassifier\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "from sklearn.neural_network import MLPClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV,RandomizedSearchCV,cross_val_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LOADING DATASET INTO DASK DATAFRAME BY FILE NAME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_loading(filename):\n",
    "    df = pd.read_csv(filename,encoding='latin-1',header=None)\n",
    "    df = df[[5,0]]\n",
    "    df.columns = ['statement','analysis']\n",
    "    df['index_col']=1\n",
    "    df['index_col'] = df['index_col'].cumsum()\n",
    "    df.dropna()\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "EXTRACTING FEATURES AND LABELS FROM DATAFRAME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_label_split(df):\n",
    "    X = df['statement']\n",
    "    y = df['analysis']\n",
    "    X = X.astype(str)\n",
    "    y = y.astype(int)\n",
    "    return X,y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "APPLYING PREPROCESSING ACTIVITIES, LEMMATIZIZNG, REMOVING STOP WORDS, TFIDF VECTORIZER \n",
    "ON FEATURE MATRIX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(sentence):\n",
    "    emoji_dict = {':)': 'smile', ':-)': 'smile', ';d': 'wink', ':-E': 'vampire', ':(': 'sad', \n",
    "                    ':-(': 'sad', ':-<': 'sad', ':P': 'raspberry', ':O': 'surprised',\n",
    "                    ':-@': 'shocked', ':@': 'shocked',':-$': 'confused', r':\\\\': 'annoyed', \n",
    "                    ':#': 'mute', ':X': 'mute', ':^)': 'smile', ':-&': 'confused', '$_$': 'greedy',\n",
    "                    '@@': 'eyeroll', ':-!': 'confused', ':-D': 'smile', ':-0': 'yell', 'O.o': 'confused',\n",
    "                    '<(-_-)>': 'robot', 'd[-_-]b': 'dj', \":'-)\": 'sadsmile', ';)': 'wink', \n",
    "                    ';-)': 'wink', 'O:-)': 'angel','O*-)': 'angel','(:-D': 'gossip', '=^.^=': 'cat'}\n",
    "    \n",
    "    sentence = str(sentence)\n",
    "    sentence = sentence.lower()\n",
    "    sentence = re.sub('<[^>]*>',' tag ',sentence)\n",
    "    sentence = re.sub(r'((http://)[^ ]*|(https://)[^ ]*|(www\\.)[^ ]*)', ' url ',sentence)\n",
    "    sentence = re.sub('@[^\\s]+>',' USER ',sentence)\n",
    "    sentence = re.sub('[^a-zA-Z0-9]',' ',sentence) \n",
    "    for emoji in emoji_dict.keys():\n",
    "        sentence = sentence.replace(emoji, \" EMOJI \"+emoji_dict[emoji])\n",
    "    sentence = re.sub(r\"(.)\\1\\1+\",r\"\\1\\1\",sentence)\n",
    "    \n",
    "    return sentence\n",
    "\n",
    "def lemmatizer(sentence):\n",
    "    return ''.join([word_lemmatizer.lemmatize(word) for word in sentence])\n",
    "\n",
    "def stop_words_remover(sentence):\n",
    "    sentence = str(sentence)\n",
    "    sentence = ''.join(sentence)\n",
    "    stopwords = nlp.Defaults.stop_words\n",
    "    new_sent = ''\n",
    "    for word_token in sentence.split():\n",
    "        if word_token not in stopwords:\n",
    "            new_sent = new_sent + word_token + ' '\n",
    "    return new_sent\n",
    "\n",
    "class DataCleaner(BaseEstimator,TransformerMixin):\n",
    "    def __init__(self,X=None,y=None):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        \n",
    "    \n",
    "    def fit(self,X,y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self,X,y=None):\n",
    "        X_ = X.copy()\n",
    "        for row in X_.iteritems():\n",
    "            row = preprocess(row)\n",
    "            row = stop_words_remover(row)\n",
    "            row = lemmatizer(row)\n",
    "        return X_\n",
    "\n",
    "tfidf = TfidfVectorizer(\n",
    "                        ngram_range=(1,2),\n",
    "                        max_features=500000,\n",
    "                        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LOADING & EXTRACTING FEATURES AND LABEL ON TRAINING, TEST DATASET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = Pipeline([\n",
    "                ('data_cleaning',DataCleaner()),\n",
    "                ('vectorizer',tfidf)\n",
    "        ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'data_loading' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32me:\\git_repos\\sentiment_analysis_demo\\model_bulder.ipynb Cell 13\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/e%3A/git_repos/sentiment_analysis_demo/model_bulder.ipynb#X15sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m df_train \u001b[39m=\u001b[39m data_loading(\u001b[39m\"\u001b[39m\u001b[39mtraining140.csv\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m      <a href='vscode-notebook-cell:/e%3A/git_repos/sentiment_analysis_demo/model_bulder.ipynb#X15sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m X_train, y_train \u001b[39m=\u001b[39m feature_label_split(df_train)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'data_loading' is not defined"
     ]
    }
   ],
   "source": [
    "df_train = data_loading(\"training140.csv\")\n",
    "X_train, y_train = feature_label_split(df_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32me:\\git_repos\\sentiment_analysis_demo\\Untitled-1.ipynb Cell 14\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/e%3A/git_repos/sentiment_analysis_demo/Untitled-1.ipynb#X16sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m X_train_tr \u001b[39m=\u001b[39m pipe\u001b[39m.\u001b[39;49mfit_transform(X_train)\n\u001b[0;32m      <a href='vscode-notebook-cell:/e%3A/git_repos/sentiment_analysis_demo/Untitled-1.ipynb#X16sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m file \u001b[39m=\u001b[39m \u001b[39mopen\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mpicklefiles/X_train.pickle\u001b[39m\u001b[39m'\u001b[39m,\u001b[39m'\u001b[39m\u001b[39mwb\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m      <a href='vscode-notebook-cell:/e%3A/git_repos/sentiment_analysis_demo/Untitled-1.ipynb#X16sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m pickle\u001b[39m.\u001b[39mdump(X_train,file)\n",
      "File \u001b[1;32mc:\\Users\\ANGELA\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\pipeline.py:414\u001b[0m, in \u001b[0;36mPipeline.fit_transform\u001b[1;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[0;32m    387\u001b[0m \u001b[39m\"\"\"Fit the model and transform with the final estimator.\u001b[39;00m\n\u001b[0;32m    388\u001b[0m \n\u001b[0;32m    389\u001b[0m \u001b[39mFits all the transformers one after the other and transform the\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    411\u001b[0m \u001b[39m    Transformed samples.\u001b[39;00m\n\u001b[0;32m    412\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    413\u001b[0m fit_params_steps \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_check_fit_params(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mfit_params)\n\u001b[1;32m--> 414\u001b[0m Xt \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_fit(X, y, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mfit_params_steps)\n\u001b[0;32m    416\u001b[0m last_step \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_final_estimator\n\u001b[0;32m    417\u001b[0m \u001b[39mwith\u001b[39;00m _print_elapsed_time(\u001b[39m\"\u001b[39m\u001b[39mPipeline\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_log_message(\u001b[39mlen\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39msteps) \u001b[39m-\u001b[39m \u001b[39m1\u001b[39m)):\n",
      "File \u001b[1;32mc:\\Users\\ANGELA\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\pipeline.py:336\u001b[0m, in \u001b[0;36mPipeline._fit\u001b[1;34m(self, X, y, **fit_params_steps)\u001b[0m\n\u001b[0;32m    334\u001b[0m     cloned_transformer \u001b[39m=\u001b[39m clone(transformer)\n\u001b[0;32m    335\u001b[0m \u001b[39m# Fit or load from cache the current transformer\u001b[39;00m\n\u001b[1;32m--> 336\u001b[0m X, fitted_transformer \u001b[39m=\u001b[39m fit_transform_one_cached(\n\u001b[0;32m    337\u001b[0m     cloned_transformer,\n\u001b[0;32m    338\u001b[0m     X,\n\u001b[0;32m    339\u001b[0m     y,\n\u001b[0;32m    340\u001b[0m     \u001b[39mNone\u001b[39;00m,\n\u001b[0;32m    341\u001b[0m     message_clsname\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mPipeline\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m    342\u001b[0m     message\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_log_message(step_idx),\n\u001b[0;32m    343\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mfit_params_steps[name],\n\u001b[0;32m    344\u001b[0m )\n\u001b[0;32m    345\u001b[0m \u001b[39m# Replace the transformer of the step with the fitted\u001b[39;00m\n\u001b[0;32m    346\u001b[0m \u001b[39m# transformer. This is necessary when loading the transformer\u001b[39;00m\n\u001b[0;32m    347\u001b[0m \u001b[39m# from the cache.\u001b[39;00m\n\u001b[0;32m    348\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msteps[step_idx] \u001b[39m=\u001b[39m (name, fitted_transformer)\n",
      "File \u001b[1;32mc:\\Users\\ANGELA\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\joblib\\memory.py:349\u001b[0m, in \u001b[0;36mNotMemorizedFunc.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    348\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m--> 349\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfunc(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\ANGELA\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\pipeline.py:870\u001b[0m, in \u001b[0;36m_fit_transform_one\u001b[1;34m(transformer, X, y, weight, message_clsname, message, **fit_params)\u001b[0m\n\u001b[0;32m    868\u001b[0m \u001b[39mwith\u001b[39;00m _print_elapsed_time(message_clsname, message):\n\u001b[0;32m    869\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mhasattr\u001b[39m(transformer, \u001b[39m\"\u001b[39m\u001b[39mfit_transform\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[1;32m--> 870\u001b[0m         res \u001b[39m=\u001b[39m transformer\u001b[39m.\u001b[39mfit_transform(X, y, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mfit_params)\n\u001b[0;32m    871\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    872\u001b[0m         res \u001b[39m=\u001b[39m transformer\u001b[39m.\u001b[39mfit(X, y, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mfit_params)\u001b[39m.\u001b[39mtransform(X)\n",
      "File \u001b[1;32mc:\\Users\\ANGELA\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\base.py:867\u001b[0m, in \u001b[0;36mTransformerMixin.fit_transform\u001b[1;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[0;32m    863\u001b[0m \u001b[39m# non-optimized default implementation; override when a better\u001b[39;00m\n\u001b[0;32m    864\u001b[0m \u001b[39m# method is possible for a given clustering algorithm\u001b[39;00m\n\u001b[0;32m    865\u001b[0m \u001b[39mif\u001b[39;00m y \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    866\u001b[0m     \u001b[39m# fit method of arity 1 (unsupervised transformation)\u001b[39;00m\n\u001b[1;32m--> 867\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfit(X, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mfit_params)\u001b[39m.\u001b[39;49mtransform(X)\n\u001b[0;32m    868\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    869\u001b[0m     \u001b[39m# fit method of arity 2 (supervised transformation)\u001b[39;00m\n\u001b[0;32m    870\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfit(X, y, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mfit_params)\u001b[39m.\u001b[39mtransform(X)\n",
      "\u001b[1;32me:\\git_repos\\sentiment_analysis_demo\\Untitled-1.ipynb Cell 14\u001b[0m in \u001b[0;36mDataCleaner.transform\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/git_repos/sentiment_analysis_demo/Untitled-1.ipynb#X16sZmlsZQ%3D%3D?line=46'>47</a>\u001b[0m     row \u001b[39m=\u001b[39m preprocess(row)\n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/git_repos/sentiment_analysis_demo/Untitled-1.ipynb#X16sZmlsZQ%3D%3D?line=47'>48</a>\u001b[0m     row \u001b[39m=\u001b[39m stop_words_remover(row)\n\u001b[1;32m---> <a href='vscode-notebook-cell:/e%3A/git_repos/sentiment_analysis_demo/Untitled-1.ipynb#X16sZmlsZQ%3D%3D?line=48'>49</a>\u001b[0m     row \u001b[39m=\u001b[39m lemmatizer(row)\n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/git_repos/sentiment_analysis_demo/Untitled-1.ipynb#X16sZmlsZQ%3D%3D?line=49'>50</a>\u001b[0m \u001b[39mreturn\u001b[39;00m X_\n",
      "\u001b[1;32me:\\git_repos\\sentiment_analysis_demo\\Untitled-1.ipynb Cell 14\u001b[0m in \u001b[0;36mlemmatizer\u001b[1;34m(sentence)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/git_repos/sentiment_analysis_demo/Untitled-1.ipynb#X16sZmlsZQ%3D%3D?line=21'>22</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mlemmatizer\u001b[39m(sentence):\n\u001b[1;32m---> <a href='vscode-notebook-cell:/e%3A/git_repos/sentiment_analysis_demo/Untitled-1.ipynb#X16sZmlsZQ%3D%3D?line=22'>23</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39m'\u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mjoin([word_lemmatizer\u001b[39m.\u001b[39mlemmatize(word) \u001b[39mfor\u001b[39;00m word \u001b[39min\u001b[39;00m sentence])\n",
      "\u001b[1;32me:\\git_repos\\sentiment_analysis_demo\\Untitled-1.ipynb Cell 14\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/git_repos/sentiment_analysis_demo/Untitled-1.ipynb#X16sZmlsZQ%3D%3D?line=21'>22</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mlemmatizer\u001b[39m(sentence):\n\u001b[1;32m---> <a href='vscode-notebook-cell:/e%3A/git_repos/sentiment_analysis_demo/Untitled-1.ipynb#X16sZmlsZQ%3D%3D?line=22'>23</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39m'\u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mjoin([word_lemmatizer\u001b[39m.\u001b[39;49mlemmatize(word) \u001b[39mfor\u001b[39;00m word \u001b[39min\u001b[39;00m sentence])\n",
      "File \u001b[1;32mc:\\Users\\ANGELA\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\nltk\\stem\\wordnet.py:45\u001b[0m, in \u001b[0;36mWordNetLemmatizer.lemmatize\u001b[1;34m(self, word, pos)\u001b[0m\n\u001b[0;32m     33\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mlemmatize\u001b[39m(\u001b[39mself\u001b[39m, word: \u001b[39mstr\u001b[39m, pos: \u001b[39mstr\u001b[39m \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mn\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mstr\u001b[39m:\n\u001b[0;32m     34\u001b[0m     \u001b[39m\"\"\"Lemmatize `word` using WordNet's built-in morphy function.\u001b[39;00m\n\u001b[0;32m     35\u001b[0m \u001b[39m    Returns the input word unchanged if it cannot be found in WordNet.\u001b[39;00m\n\u001b[0;32m     36\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     43\u001b[0m \u001b[39m    :return: The lemma of `word`, for the given `pos`.\u001b[39;00m\n\u001b[0;32m     44\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m---> 45\u001b[0m     lemmas \u001b[39m=\u001b[39m wn\u001b[39m.\u001b[39;49m_morphy(word, pos)\n\u001b[0;32m     46\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mmin\u001b[39m(lemmas, key\u001b[39m=\u001b[39m\u001b[39mlen\u001b[39m) \u001b[39mif\u001b[39;00m lemmas \u001b[39melse\u001b[39;00m word\n",
      "File \u001b[1;32mc:\\Users\\ANGELA\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\nltk\\corpus\\reader\\wordnet.py:2036\u001b[0m, in \u001b[0;36mWordNetCorpusReader._morphy\u001b[1;34m(self, form, pos, check_exceptions)\u001b[0m\n\u001b[0;32m   2033\u001b[0m         \u001b[39mreturn\u001b[39;00m filter_forms([form] \u001b[39m+\u001b[39m exceptions[form])\n\u001b[0;32m   2035\u001b[0m \u001b[39m# 1. Apply rules once to the input to get y1, y2, y3, etc.\u001b[39;00m\n\u001b[1;32m-> 2036\u001b[0m forms \u001b[39m=\u001b[39m apply_rules([form])\n\u001b[0;32m   2038\u001b[0m \u001b[39m# 2. Return all that are in the database (and check the original too)\u001b[39;00m\n\u001b[0;32m   2039\u001b[0m results \u001b[39m=\u001b[39m filter_forms([form] \u001b[39m+\u001b[39m forms)\n",
      "File \u001b[1;32mc:\\Users\\ANGELA\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\nltk\\corpus\\reader\\wordnet.py:2012\u001b[0m, in \u001b[0;36mWordNetCorpusReader._morphy.<locals>.apply_rules\u001b[1;34m(forms)\u001b[0m\n\u001b[0;32m   2011\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mapply_rules\u001b[39m(forms):\n\u001b[1;32m-> 2012\u001b[0m     \u001b[39mreturn\u001b[39;00m [\n\u001b[0;32m   2013\u001b[0m         form[: \u001b[39m-\u001b[39m\u001b[39mlen\u001b[39m(old)] \u001b[39m+\u001b[39m new\n\u001b[0;32m   2014\u001b[0m         \u001b[39mfor\u001b[39;00m form \u001b[39min\u001b[39;00m forms\n\u001b[0;32m   2015\u001b[0m         \u001b[39mfor\u001b[39;00m old, new \u001b[39min\u001b[39;00m substitutions\n\u001b[0;32m   2016\u001b[0m         \u001b[39mif\u001b[39;00m form\u001b[39m.\u001b[39mendswith(old)\n\u001b[0;32m   2017\u001b[0m     ]\n",
      "File \u001b[1;32mc:\\Users\\ANGELA\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\nltk\\corpus\\reader\\wordnet.py:2015\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m   2011\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mapply_rules\u001b[39m(forms):\n\u001b[0;32m   2012\u001b[0m     \u001b[39mreturn\u001b[39;00m [\n\u001b[0;32m   2013\u001b[0m         form[: \u001b[39m-\u001b[39m\u001b[39mlen\u001b[39m(old)] \u001b[39m+\u001b[39m new\n\u001b[0;32m   2014\u001b[0m         \u001b[39mfor\u001b[39;00m form \u001b[39min\u001b[39;00m forms\n\u001b[1;32m-> 2015\u001b[0m         \u001b[39mfor\u001b[39;00m old, new \u001b[39min\u001b[39;00m substitutions\n\u001b[0;32m   2016\u001b[0m         \u001b[39mif\u001b[39;00m form\u001b[39m.\u001b[39mendswith(old)\n\u001b[0;32m   2017\u001b[0m     ]\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "X_train_tr = pipe.fit_transform(X_train)\n",
    "\n",
    "file = open('picklefiles/X_train.pickle','wb')\n",
    "pickle.dump(X_train,file)\n",
    "file.close()\n",
    "\n",
    "file = open('picklefiles/pipe_fitted.pickle','wb')\n",
    "pickle.dump(pipe,file)\n",
    "file.close()\n",
    "\n",
    "file = open('picklefiles/y_train.pickle','wb')\n",
    "pickle.dump(y_train,file)\n",
    "file.close() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = pickle.load(open(\"picklefiles/X_train.pickle\",'rb'))\n",
    "X_train_tr = pickle.load(open(\"picklefiles/X_train_tr.pickle\",'rb'))\n",
    "y_train = pickle.load(open(\"picklefiles/y_train.pickle\",'rb'))\n",
    "pipe = pickle.load(open('picklefiles/pipe_fitted.pickle','rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = data_loading(\"test140.csv\")\n",
    "X_test, y_test = feature_label_split(df_test)\n",
    "X_test_tr =pipe.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CREATING CLASSIFIERS, PARAMETER GRID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf1 = LinearSVC(max_iter=1000,tol=0.0001)\n",
    "\n",
    "param_grid1 =   {'C':[0.001,0.01,0.1,1.0,10.0],\n",
    "                }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf2 = SVC(max_iter=1000,tol=0.001,cache_size=5)\n",
    "param_grid2 =   {'C':[0.001,0.01,0.1,1.0,10.0],\n",
    "                'gamma':['auto','scale'],\n",
    "                'kernel':['poly','rbf','sigmoid']\n",
    "                }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf3 = LogisticRegression(max_iter=1000,tol=0.0001,warm_start=True,n_jobs=-1,solver='saga')\n",
    "param_grid3 = {'C':[0.001,0.01,0.1,1.0,10.0],\n",
    "                'solver' : ['sag','lbfgs']\n",
    "            }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf4 = DecisionTreeClassifier(max_depth=10,min_samples_split=2)\n",
    "param_grid4 = {'max_depth' : range(5,25),\n",
    "                'min_samples_split' : range(2,8),\n",
    "                'min_samples_leaf' : range(1,8)\n",
    "                }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf5 = MultinomialNB()\n",
    "param_grid5 = {'alpha':[0.001,0.1,1,10,100]\n",
    "                }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "linsvcCV = pickle.load(open('picklefiles/linearsvc_best_model.pickle','rb'))\n",
    "logregCV = pickle.load(open('picklefiles/logreg_best_model.pickle','rb'))\n",
    "mulinomialnbCV = pickle.load(open('picklefiles/multinomialnb_best_model.pickle','rb'))\n",
    "desctreeCV = pickle.load(open('picklefiles/desctree_best_model.pickle','rb'))\n",
    "\n",
    "estimators = [('lsvc',linsvcCV.best_estimator_),\n",
    "               ('lr',logregCV.best_estimator_),\n",
    "               ('mnb',mulinomialnbCV.best_estimator_),\n",
    "               ('dt',desctreeCV.best_estimator_) \n",
    "                ]\n",
    "\n",
    "clf6 = VotingClassifier(estimators,voting='hard',n_jobs=-1)\n",
    "param_grid6 = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf7 = BaggingClassifier(base_estimator=linsvcCV.best_estimator_,n_jobs=-1)\n",
    "param_grid7 = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf8 = RandomForestClassifier(n_estimators=20,max_features='sqrt',max_depth=50,min_samples_split=8,min_samples_leaf=4,n_jobs=-1)\n",
    "param_grid8 = { 'max_depth':np.linspace(start=20,stop=200,num=10).astype(int),\n",
    "                'min_samples_split':[2,4,6,8,10],\n",
    "                'min_samples_leaf':[2,4,6,8,10]\n",
    "                }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf9 = AdaBoostClassifier(n_estimators=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf10 = XGBClassifier(n_estimators=200,max_depth=25,learning_rate=0.5,booster='gbtree',n_jobs=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf11 = GradientBoostingClassifier(n_estimators=200,learning_rate=0.5,max_depth=25,max_features='sqrt',warm_start=True,tol=0.0012)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf12 = MLPClassifier(hidden_layer_sizes=(50,100,10),tol=0.0001,learning_rate_init=0.005,verbose=True)\n",
    "#clf12 = MLPClassifier(hidden_layer_sizes=(32,128,8,1),tol=0.0001,verbose=True,learning_rate_init=0.005,n_iter_no_change=10)\n",
    "#clf12 = MLPClassifier(hidden_layer_sizes=(50),tol=0.0001,verbose=True,learning_rate_init=0.005,n_iter_no_change=10,batch_size=1024)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "COMPUTING ACCURACY SCORE AS MODEL METRIC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_accuracy(model,X,y):\n",
    "    y_predictions = model.predict(X)\n",
    "    ac_score = accuracy_score(y,y_predictions)\n",
    "    return ac_score\n",
    "\n",
    "def compute_accuracy1(model,X,y):\n",
    "    y_predictions = model.predict(X)\n",
    "    y = y.apply(lambda x:0 if x==0 else 1)\n",
    "    y.astype(int)\n",
    "    ac_score = accuracy_score(y,y_predictions)\n",
    "    return ac_score    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TRAINING LINEARSVC CLASSIFIER ON TRAINING SET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ANGELA\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:709: UserWarning: Training interrupted by user.\n",
      "  warnings.warn(\"Training interrupted by user.\")\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-8 {color: black;background-color: white;}#sk-container-id-8 pre{padding: 0;}#sk-container-id-8 div.sk-toggleable {background-color: white;}#sk-container-id-8 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-8 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-8 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-8 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-8 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-8 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-8 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-8 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-8 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-8 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-8 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-8 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-8 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-8 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-8 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-8 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-8 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-8 div.sk-item {position: relative;z-index: 1;}#sk-container-id-8 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-8 div.sk-item::before, #sk-container-id-8 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-8 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-8 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-8 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-8 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-8 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-8 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-8 div.sk-label-container {text-align: center;}#sk-container-id-8 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-8 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-8\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>MLPClassifier(hidden_layer_sizes=(50, 100, 10), learning_rate_init=0.005,\n",
       "              verbose=True)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-8\" type=\"checkbox\" checked><label for=\"sk-estimator-id-8\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">MLPClassifier</label><div class=\"sk-toggleable__content\"><pre>MLPClassifier(hidden_layer_sizes=(50, 100, 10), learning_rate_init=0.005,\n",
       "              verbose=True)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "MLPClassifier(hidden_layer_sizes=(50, 100, 10), learning_rate_init=0.005,\n",
       "              verbose=True)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#y_train = y_train.apply(lambda x: 0 if x==0 else 1)\n",
    "clf12.fit(X_train_tr,y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PRINTING ACCURACY SCORE OF LINEARSVC ESTIMATOR ON TRAINING SET FOLLOWED BY TEST SET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.762021875\n",
      "0.5742971887550201\n"
     ]
    }
   ],
   "source": [
    "print(compute_accuracy(clf12,X_train_tr,y_train))\n",
    "print(compute_accuracy(clf12,X_test_tr,y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = open('picklefiles/mlpclf_model.pickle','wb')\n",
    "pickle.dump(clf12,file)\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "HYPERPARAMETER TUNING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_search_svc = GridSearchCV(estimator = clf9, \n",
    "                                        param_grid=param_grid9,\n",
    "                                        cv=3,\n",
    "                                        refit= 'acc',\n",
    "                                        scoring={'acc':'accuracy',\n",
    "                                                 'mse':'neg_mean_squared_error'\n",
    "                                                },\n",
    "                                        n_jobs=-1,\n",
    "                                        pre_dispatch='2*n_jobs',\n",
    "                                        return_train_score=True\n",
    "                                )  \n",
    "\n",
    "rand_search_svc = RandomizedSearchCV(estimator = clf9, \n",
    "                                        param_distributions=param_grid9,\n",
    "                                        cv=3,\n",
    "                                        refit= 'acc',\n",
    "                                        scoring={'acc':'accuracy',\n",
    "                                                 'mse':'neg_mean_squared_error'\n",
    "                                                },\n",
    "                                        n_jobs=-1,\n",
    "                                        pre_dispatch='2*n_jobs',\n",
    "                                        return_train_score=True\n",
    "                                ) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#grid_search_svc.fit(X_train_tr,y_train)\n",
    "file = open('picklefiles/adaboost_best_model.pickle','wb')\n",
    "pickle.dump(clf9,file)\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_search_svc.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(compute_accuracy(rand_search_svc.best_estimator_,X_train_tr,y_train))\n",
    "print(compute_accuracy(rand_search_svc.best_estimator_,X_test_tr,y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "linearsvcCV = pickle.load(open('picklefiles/linearsvc_best_model.pickle','rb'))\n",
    "linearsvc = linearsvcCV.best_estimator_\n",
    "\n",
    "svcCV = pickle.load(open('picklefiles/svc_best_model.pickle','rb'))\n",
    "svcclf = svcCV.best_estimator_\n",
    "\n",
    "logregCV = pickle.load(open('picklefiles/logreg_best_model.pickle','rb'))\n",
    "logreg = logregCV.best_estimator_\n",
    "\n",
    "multinomialnbCV = pickle.load(open('picklefiles/multinomialnb_best_model.pickle','rb'))\n",
    "multinomialnb = multinomialnbCV.best_estimator_\n",
    "\n",
    "mvotingCV = pickle.load(open('picklefiles/voting_best_model.pickle','rb'))\n",
    "mvoting = mvotingCV.best_estimator_\n",
    "\n",
    "baggingCV = pickle.load(open('picklefiles/bagging_best_model.pickle','rb'))\n",
    "bagging = baggingCV\n",
    "\n",
    "rforestCV = pickle.load(open('picklefiles/randforest_best_model.pickle','rb'))\n",
    "rforest = rforestCV.best_estimator_\n",
    "\n",
    "adaboost = pickle.load(open('picklefiles/adaboost_model.pickle','rb'))\n",
    "\n",
    "xgboost = pickle.load(open('picklefiles/xgboost_model.pickle','rb'))\n",
    "\n",
    "gradientboost = pickle.load(open('picklefiles/gradientboost_model.pickle','rb'))\n",
    "\n",
    "mlpclf = pickle.load(open('picklefiles/mlpclf_model.pickle','rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "SVC: SVC(C=10.0, cache_size=5, kernel='sigmoid', max_iter=1000)\n",
      "Training set accuracy:  0.5995675\n",
      "Test set accuracy:  0.43373493975903615\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nSVC:\",svcclf)\n",
    "print('Training set accuracy: ',compute_accuracy(svcclf,X_train_tr,y_train))\n",
    "print('Test set accuracy: ',compute_accuracy(svcclf,X_test_tr,y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "LinearSVC: LinearSVC(C=0.1)\n",
      "Training set accuracy:  0.858434375\n",
      "Test set accuracy:  0.5963855421686747\n",
      "\n",
      "LogisticRegressor: LogisticRegression(max_iter=1000, n_jobs=-1, warm_start=True)\n",
      "Training set accuracy:  0.8566475\n",
      "Test set accuracy:  0.5983935742971888\n",
      "\n",
      "MultinomialNB: MultinomialNB(alpha=1)\n",
      "Training set accuracy:  0.8386875\n",
      "Test set accuracy:  0.608433734939759\n",
      "\n",
      "VotingClassifier: VotingClassifier(estimators=[('lsvc', LinearSVC(C=0.1)),\n",
      "                             ('lr',\n",
      "                              LogisticRegression(max_iter=1000, n_jobs=-1,\n",
      "                                                 warm_start=True)),\n",
      "                             ('mnb', MultinomialNB(alpha=1)),\n",
      "                             ('dt',\n",
      "                              DecisionTreeClassifier(max_depth=23,\n",
      "                                                     min_samples_leaf=5,\n",
      "                                                     min_samples_split=4))],\n",
      "                 n_jobs=-1)\n",
      "Training set accuracy:  0.855020625\n",
      "Test set accuracy:  0.5943775100401606\n",
      "\n",
      "BaggingClassifier: BaggingClassifier(base_estimator=LinearSVC(C=0.1), n_jobs=-1)\n",
      "Training set accuracy:  0.85700875\n",
      "Test set accuracy:  0.5943775100401606\n",
      "\n",
      "RandomForestClassifier: RandomForestClassifier(max_depth=200, min_samples_leaf=2, min_samples_split=8,\n",
      "                       n_estimators=20, n_jobs=-1)\n",
      "Training set accuracy:  0.795239375\n",
      "Test set accuracy:  0.5481927710843374\n",
      "\n",
      "AdaBoostClassifier: AdaBoostClassifier(n_estimators=200)\n",
      "Training set accuracy:  0.7463475\n",
      "Test set accuracy:  0.5341365461847389\n",
      "\n",
      "XGBClassifier: XGBClassifier(base_score=0.5, booster='gbtree', callbacks=None,\n",
      "              colsample_bylevel=1, colsample_bynode=1, colsample_bytree=1,\n",
      "              early_stopping_rounds=None, enable_categorical=False,\n",
      "              eval_metric=None, gamma=0, gpu_id=-1, grow_policy='depthwise',\n",
      "              importance_type=None, interaction_constraints='',\n",
      "              learning_rate=0.5, max_bin=256, max_cat_to_onehot=4,\n",
      "              max_delta_step=0, max_depth=25, max_leaves=0, min_child_weight=1,\n",
      "              missing=nan, monotone_constraints='()', n_estimators=200,\n",
      "              n_jobs=-1, num_parallel_tree=1, predictor='auto', random_state=0,\n",
      "              reg_alpha=0, reg_lambda=1, ...)\n",
      "Training set accuracy:  0.895970625\n",
      "Test set accuracy:  0.8293172690763052\n",
      "\n",
      "GradientBoostingClassifier: GradientBoostingClassifier(learning_rate=0.5, max_depth=25, max_features='sqrt',\n",
      "                           n_estimators=200, tol=0.0012, warm_start=True)\n",
      "Training set accuracy:  0.84224\n",
      "Test set accuracy:  0.8293172690763052\n",
      "\n",
      "MLPClassifier: MLPClassifier(hidden_layer_sizes=(50, 100, 10), learning_rate_init=0.005,\n",
      "              verbose=True)\n",
      "Training set accuracy:  0.762021875\n",
      "Test set accuracy:  0.5742971887550201\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nLinearSVC:\",linearsvc)\n",
    "print('Training set accuracy: ',compute_accuracy(linearsvc,X_train_tr,y_train))\n",
    "print('Test set accuracy: ',compute_accuracy(linearsvc,X_test_tr,y_test))\n",
    "print(\"\\nLogisticRegressor:\",logreg)\n",
    "print('Training set accuracy: ',compute_accuracy(logreg,X_train_tr,y_train))\n",
    "print('Test set accuracy: ',compute_accuracy(logreg,X_test_tr,y_test))\n",
    "print(\"\\nMultinomialNB:\",multinomialnb)\n",
    "print('Training set accuracy: ',compute_accuracy(multinomialnb,X_train_tr,y_train))\n",
    "print('Test set accuracy: ',compute_accuracy(multinomialnb,X_test_tr,y_test))\n",
    "print(\"\\nVotingClassifier:\",mvoting)\n",
    "print('Training set accuracy: ',compute_accuracy(mvoting,X_train_tr,y_train))\n",
    "print('Test set accuracy: ',compute_accuracy(mvoting,X_test_tr,y_test))\n",
    "print(\"\\nBaggingClassifier:\",bagging)\n",
    "print('Training set accuracy: ',compute_accuracy(bagging,X_train_tr,y_train))\n",
    "print('Test set accuracy: ',compute_accuracy(bagging,X_test_tr,y_test))\n",
    "print(\"\\nRandomForestClassifier:\",rforest)\n",
    "print('Training set accuracy: ',compute_accuracy(rforest,X_train_tr,y_train))\n",
    "print('Test set accuracy: ',compute_accuracy(rforest,X_test_tr,y_test))\n",
    "print(\"\\nAdaBoostClassifier:\",adaboost)\n",
    "print('Training set accuracy: ',compute_accuracy(adaboost,X_train_tr,y_train))\n",
    "print('Test set accuracy: ',compute_accuracy(adaboost,X_test_tr,y_test))\n",
    "print(\"\\nXGBClassifier:\",xgboost)\n",
    "print('Training set accuracy: ',compute_accuracy1(xgboost,X_train_tr,y_train))\n",
    "print('Test set accuracy: ',compute_accuracy1(xgboost,X_test_tr,y_test))\n",
    "print(\"\\nGradientBoostingClassifier:\",gradientboost)\n",
    "print('Training set accuracy: ',compute_accuracy1(gradientboost,X_train_tr,y_train))\n",
    "print('Test set accuracy: ',compute_accuracy1(gradientboost,X_test_tr,y_test))\n",
    "print(\"\\nMLPClassifier:\",mlpclf)\n",
    "print('Training set accuracy: ',compute_accuracy(mlpclf,X_train_tr,y_train))\n",
    "print('Test set accuracy: ',compute_accuracy(mlpclf,X_test_tr,y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "55e1bd3b0b077e0bc98218d3177b556cf36523ce0b56978437ca49173cd0e5fd"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
